{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training pipeline for arutema47 model.\n",
    "\n",
    "The model and image tile preparation is based on public kernels.\n",
    "\n",
    "* Most important part is using the denoised training labels.\n",
    "* model efficientnet-b0\n",
    "* Original augumentations for better generization (specifics in the dataloader sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_type = 'efficientnet-b0'\n",
    "modelname = kernel_type\n",
    "\n",
    "fold = 0\n",
    "tile_size = 256\n",
    "image_size = 256\n",
    "n_tiles = 36\n",
    "batch_size = 3\n",
    "num_workers = 8\n",
    "out_dim = 5\n",
    "init_lr = 1e-4\n",
    "warmup_factor = 10\n",
    "n_epochs = 30\n",
    "\n",
    "load_raw_png = False\n",
    "load_jpg = False\n",
    "\n",
    "mixup = True\n",
    "cutmix = False\n",
    "\n",
    "# Cosine annealing or exp scheduler\n",
    "COSINE = True\n",
    "EXP = False\n",
    "if not COSINE:\n",
    "    EXP = True\n",
    "\n",
    "poolmethod = \"avgpool\"\n",
    "\n",
    "kernel_type += \"famlabelsmodelsub_{}_tile{}_imsize{}\".format(poolmethod, n_tiles, image_size)\n",
    "if mixup:\n",
    "    kernel_type += \"_mixup\"\n",
    "if cutmix:\n",
    "    kernel_type += \"_cutmix\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# make dirs\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"log\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import skimage.io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, RandomSampler, SequentialSampler\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "#from efficientnet_pytorch import model as enet\n",
    "import albumentations\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix SEED\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We use the denoised training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/val/Downloads/Kaggle-PANDA-1st-place-solution\n",
      "./input/train_256_36\n"
     ]
    }
   ],
   "source": [
    "data_dir = './input/'\n",
    "# load clean labels\n",
    "df_train = pd.read_csv(\"./output/model/final_1/local_preds_final_1_efficientnet-b1_removed_noise_thresh_16.csv\")\n",
    "image_folder = os.path.join(data_dir, 'train_256_36')\n",
    "\n",
    "warmup_epo = 1\n",
    "\n",
    "df_train = df_train.sample(100).reset_index(drop=True) if DEBUG else df_train\n",
    "\n",
    "device = torch.device('cuda')\n",
    "print(os.getcwd())\n",
    "print(image_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "      <th>isup_grade</th>\n",
       "      <th>gleason_score</th>\n",
       "      <th>kfold</th>\n",
       "      <th>probs_raw</th>\n",
       "      <th>probs_without_tta_raw</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_without_tta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001d865e65ef5d2579c190a0e0350d8f</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.020964</td>\n",
       "      <td>0.025039</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>003a91841da04a5a31f808fb5c21538a</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>1</td>\n",
       "      <td>3+3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.042482</td>\n",
       "      <td>1.044420</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>004dd32d9cd167d9cc31c13b704498af</td>\n",
       "      <td>radboud</td>\n",
       "      <td>1</td>\n",
       "      <td>3+3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.163072</td>\n",
       "      <td>1.184651</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004f6b3a66189b4e88b6a01ba19d7d31</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>1</td>\n",
       "      <td>3+3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.210813</td>\n",
       "      <td>2.346966</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00a26aaa82c959624d90dfb69fcf259c</td>\n",
       "      <td>radboud</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "      <td>1</td>\n",
       "      <td>3.559884</td>\n",
       "      <td>3.723556</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           image_id data_provider  isup_grade gleason_score  \\\n",
       "0  001d865e65ef5d2579c190a0e0350d8f    karolinska           0           0+0   \n",
       "1  003a91841da04a5a31f808fb5c21538a    karolinska           1           3+3   \n",
       "2  004dd32d9cd167d9cc31c13b704498af       radboud           1           3+3   \n",
       "3  004f6b3a66189b4e88b6a01ba19d7d31    karolinska           1           3+3   \n",
       "4  00a26aaa82c959624d90dfb69fcf259c       radboud           4           4+4   \n",
       "\n",
       "   kfold  probs_raw  probs_without_tta_raw  preds  preds_without_tta  \n",
       "0      1   0.020964               0.025039      0                  0  \n",
       "1      1   1.042482               1.044420      1                  1  \n",
       "2      1   1.163072               1.184651      1                  1  \n",
       "3      1   2.210813               2.346966      2                  2  \n",
       "4      1   3.559884               3.723556      4                  4  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def erase(df_train):\n",
    "    df_train2 = df_train\n",
    "    erase = []\n",
    "    for i, id in enumerate(df_train2[\"image_id\"].to_numpy()):\n",
    "        if not os.path.isfile(os.path.join(image_folder, f'{id}.npz')):\n",
    "            erase.append(i)\n",
    "            pass\n",
    "        #img = cv2.imread(os.path.join(image_folder, f'{id}.png'))\n",
    "        \n",
    "    return df_train.drop(erase)\n",
    "\n",
    "df_train = erase(df_train).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10013"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(\"index\", 1)\n",
    "len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>data_provider</th>\n",
       "      <th>isup_grade</th>\n",
       "      <th>gleason_score</th>\n",
       "      <th>kfold</th>\n",
       "      <th>probs_raw</th>\n",
       "      <th>probs_without_tta_raw</th>\n",
       "      <th>preds</th>\n",
       "      <th>preds_without_tta</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10008</th>\n",
       "      <td>ff228af5af1389e09e293692ea479d98</td>\n",
       "      <td>radboud</td>\n",
       "      <td>4</td>\n",
       "      <td>4+4</td>\n",
       "      <td>5</td>\n",
       "      <td>4.365540</td>\n",
       "      <td>4.469392</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10009</th>\n",
       "      <td>ff30f31f767ae4d18f5145598f46f5fd</td>\n",
       "      <td>radboud</td>\n",
       "      <td>5</td>\n",
       "      <td>4+5</td>\n",
       "      <td>5</td>\n",
       "      <td>3.728697</td>\n",
       "      <td>3.714015</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10010</th>\n",
       "      <td>ff328f9aacb5c8844fb3907c5da69c77</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.211002</td>\n",
       "      <td>0.195072</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10011</th>\n",
       "      <td>ff602bd9652640d4b534d7a78fc64b6a</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.024534</td>\n",
       "      <td>0.028135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10012</th>\n",
       "      <td>ff79f700dc97b2fd05d92e56f4b4a815</td>\n",
       "      <td>karolinska</td>\n",
       "      <td>0</td>\n",
       "      <td>0+0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.178487</td>\n",
       "      <td>0.174824</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               image_id data_provider  isup_grade  \\\n",
       "10008  ff228af5af1389e09e293692ea479d98       radboud           4   \n",
       "10009  ff30f31f767ae4d18f5145598f46f5fd       radboud           5   \n",
       "10010  ff328f9aacb5c8844fb3907c5da69c77    karolinska           0   \n",
       "10011  ff602bd9652640d4b534d7a78fc64b6a    karolinska           0   \n",
       "10012  ff79f700dc97b2fd05d92e56f4b4a815    karolinska           0   \n",
       "\n",
       "      gleason_score  kfold  probs_raw  probs_without_tta_raw  preds  \\\n",
       "10008           4+4      5   4.365540               4.469392      4   \n",
       "10009           4+5      5   3.728697               3.714015      4   \n",
       "10010           0+0      5   0.211002               0.195072      0   \n",
       "10011           0+0      5   0.024534               0.028135      0   \n",
       "10012           0+0      5   0.178487               0.174824      0   \n",
       "\n",
       "       preds_without_tta  fold  \n",
       "10008                  4     2  \n",
       "10009                  4     0  \n",
       "10010                  0     2  \n",
       "10011                  0     2  \n",
       "10012                  0     1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(5, shuffle=True, random_state=42)\n",
    "df_train['fold'] = -1\n",
    "for i, (train_idx, valid_idx) in enumerate(skf.split(df_train, df_train['isup_grade'])):\n",
    "    df_train.loc[valid_idx, 'fold'] = i\n",
    "\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx = np.where((df_train['fold'] != fold))[0]\n",
    "valid_idx = np.where((df_train['fold'] == fold))[0]\n",
    "\n",
    "df_this  = df_train.loc[train_idx]\n",
    "df_valid = df_train.loc[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model.\n",
    "\n",
    "The model is based on a public kernel:\n",
    "\n",
    "https://www.kaggle.com/haqishen/train-efficientnet-b0-w-36-tiles-256-lb0-87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.ops import misc as misc_nn_ops\n",
    "from torchvision.models import resnet\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "class enetv2(nn.Module):\n",
    "    def __init__(self, out_dim=5, freeze_bn=True):\n",
    "        super(enetv2, self).__init__()\n",
    "        self.basemodel = EfficientNet.from_pretrained(modelname) \n",
    "        self.myfc = nn.Linear(self.basemodel._fc.in_features, out_dim)\n",
    "        self.basemodel._fc = nn.Identity()        \n",
    "            \n",
    "    def extract(self, x):\n",
    "        return self.basemodel(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.basemodel(x)\n",
    "        x = self.myfc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "enetv2(\n",
       "  (basemodel): EfficientNet(\n",
       "    (_conv_stem): Conv2dStaticSamePadding(\n",
       "      3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "      (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "    )\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (6): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (7): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (9): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (10): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (12): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (13): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (14): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): MemoryEfficientSwish()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_fc): Identity()\n",
       "    (_swish): MemoryEfficientSwish()\n",
       "  )\n",
       "  (myfc): Linear(in_features=1280, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enetv2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Tile preparation is also based on the public kernel.\n",
    "https://www.kaggle.com/haqishen/train-efficientnet-b0-w-36-tiles-256-lb0-87\n",
    "\n",
    "\n",
    "Augumentations are different and important.\n",
    "\n",
    "* We add cutouts, rotations augumentation for better generization.\n",
    "* We add mixup augumentation for training for better generization.\n",
    "\n",
    "mixup: Beyond Empirical Risk Minimization\n",
    "\n",
    "https://arxiv.org/abs/1710.09412\n",
    "\n",
    "For mixup, I simply made an another tile and mixed two images.\n",
    "I just implemented mixup from the original paper, but some implementation can be found from Bengali contests.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[1]\n",
    "    H = size[2]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiles(img, mode=0, transform=None):\n",
    "        result = []\n",
    "        h, w, c = img.shape\n",
    "        pad_h = (tile_size - h % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "        pad_w = (tile_size - w % tile_size) % tile_size + ((tile_size * mode) // 2)\n",
    "\n",
    "        img2 = np.pad(img,[[pad_h // 2, pad_h - pad_h // 2], [pad_w // 2,pad_w - pad_w//2], [0,0]], constant_values=255)\n",
    "        img3 = img2.reshape(\n",
    "            img2.shape[0] // tile_size,\n",
    "            tile_size,\n",
    "            img2.shape[1] // tile_size,\n",
    "            tile_size,\n",
    "            3\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        img3 = img3.transpose(0,2,1,3,4).reshape(-1, tile_size, tile_size,3)\n",
    "        n_tiles_with_info = (img3.reshape(img3.shape[0],-1).sum(1) < tile_size ** 2 * 3 * 255).sum()\n",
    "        if len(img3) < n_tiles:\n",
    "            img3 = np.pad(img3,[[0,n_tiles-len(img3)],[0,0],[0,0],[0,0]], constant_values=255)\n",
    "        idxs = np.argsort(img3.reshape(img3.shape[0],-1).sum(-1))[:n_tiles]\n",
    "        img3 = img3[idxs]\n",
    "        img3 = (img3*255).astype(np.uint8)\n",
    "        for i in range(len(img3)):\n",
    "            if transform is not None:\n",
    "                img3[i] = transform(image=img3[i])['image']\n",
    "            result.append({'img':img3[i], 'idx':i})\n",
    "        return result, n_tiles_with_info >= n_tiles\n",
    "\n",
    "\n",
    "class PANDADataset(Dataset):\n",
    "    def __init__(self,\n",
    "                 df,\n",
    "                 image_size,\n",
    "                 n_tiles=n_tiles,\n",
    "                 tile_mode=0,\n",
    "                 rand=False,\n",
    "                 transform=None,\n",
    "                ):\n",
    "\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.image_size = image_size\n",
    "        self.n_tiles = n_tiles\n",
    "        self.tile_mode = tile_mode\n",
    "        self.rand = rand\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        img_id = row.image_id\n",
    "        \n",
    "        if load_raw_png:\n",
    "            tiff_file = os.path.join(image_folder, f'{img_id}.jpg')\n",
    "            image = cv2.imread(tiff_file)\n",
    "            if self.transform is not None:\n",
    "                tiles, OK = get_tiles(image, self.tile_mode, self.transform)\n",
    "            else:\n",
    "                tiles, OK = get_tiles(image, self.tile_mode)\n",
    "\n",
    "            if self.rand:\n",
    "                idxes = np.random.choice(list(range(self.n_tiles)), self.n_tiles, replace=False)\n",
    "            else:\n",
    "                idxes = list(range(self.n_tiles))\n",
    "\n",
    "            n_row_tiles = int(np.sqrt(self.n_tiles))\n",
    "            images = np.zeros((image_size * n_row_tiles, image_size * n_row_tiles, 3))\n",
    "            for h in range(n_row_tiles):\n",
    "                for w in range(n_row_tiles):\n",
    "                    i = h * n_row_tiles + w\n",
    "\n",
    "                    if len(tiles) > idxes[i]:\n",
    "                        this_img = tiles[idxes[i]]['img']\n",
    "                    else:\n",
    "                        this_img = np.ones((self.image_size, self.image_size, 3)).astype(np.uint8) * 255\n",
    "                    this_img = 255 - this_img\n",
    "                    if self.transform is not None:\n",
    "                        this_img = self.transform(image=this_img)['image']\n",
    "                    h1 = h * image_size\n",
    "                    w1 = w * image_size\n",
    "                    images[h1:h1+image_size, w1:w1+image_size] = this_img\n",
    "                    images = (images*255).astype(np.uint8)\n",
    "        elif load_jpg:            \n",
    "            file = os.path.join(\"train_{}_{}_aug\".format(self.image_size, self.n_tiles), f'{img_id}_{np.random.randint(0,9)}.jpg')\n",
    "            images = cv2.imread(file)\n",
    "        else:\n",
    "            file = os.path.join(\"./input/train_{}_{}\".format(self.image_size, self.n_tiles), f'{img_id}.npz')\n",
    "            images = np.load(file)[\"arr_0\"]\n",
    "            images = images.transpose(2, 0, 1)\n",
    "        \n",
    "        # Load labels\n",
    "        label = np.zeros(5).astype(np.float32)\n",
    "        label[:row.isup_grade] = 1.\n",
    "        \n",
    "        # aug\n",
    "        if self.transform is not None:\n",
    "            images = self.transform(image=images)['image']\n",
    "        \n",
    "        # Mixup part\n",
    "        rd = np.random.rand()\n",
    "        if mixup and rd < 0.3 and self.transform is not None:\n",
    "            mix_idx = np.random.random_integers(0, len(self.df))\n",
    "            row2 = self.df.iloc[mix_idx]\n",
    "            img_id2 = row2.image_id\n",
    "            file = os.path.join(\"./input/train_{}_{}\".format(self.image_size, self.n_tiles), f'{img_id2}.npz')\n",
    "            images2 = np.load(file)[\"arr_0\"]\n",
    "            images2 = images2.transpose(2, 0, 1)\n",
    "            \n",
    "            if self.transform is not None:\n",
    "                images2 = self.transform(image=images2)['image']\n",
    "            \n",
    "            # blend image\n",
    "            gamma = np.random.beta(1,1)\n",
    "            images = ((images*gamma + images2*(1-gamma))).astype(np.uint8)\n",
    "            # blend labels\n",
    "            label2 = np.zeros(5).astype(np.float32)\n",
    "            label2[:row2.isup_grade] = 1.\n",
    "            label = (label*gamma+label2*(1-gamma))\n",
    "        \n",
    "        # Also tried cutmix, does not work well. Maybe because tile includes important information partially.\n",
    "        elif cutmix and rd > 0.7 and self.transform is not None:\n",
    "            #print(\"cutmix\")\n",
    "            mix_idx = np.random.random_integers(0, len(self.df))\n",
    "            row2 = self.df.iloc[mix_idx]\n",
    "            img_id2 = row2.image_id\n",
    "            file = os.path.join(\"train_{}_{}\".format(self.image_size, self.n_tiles), f'{img_id2}.npz')\n",
    "            images2 = np.load(file)[\"arr_0\"]\n",
    "            images2 = images2.transpose(2, 0, 1)\n",
    "            \n",
    "            # blend image\n",
    "            gamma = np.random.beta(1,1)\n",
    "            bbx1, bby1, bbx2, bby2 = rand_bbox(images.shape, gamma)\n",
    "            images[:, bbx1:bbx2, bby1:bby2] = images2[:, bbx1:bbx2, bby1:bby2]\n",
    "            # adjust lambda to exactly match pixel ratio\n",
    "            gamma = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (images.shape[-1] * images.shape[-2]))\n",
    "            # blend labels\n",
    "            label2 = np.zeros(5).astype(np.float32)\n",
    "            label2[:row2.isup_grade] = 1.\n",
    "            label = (label*gamma+label2*(1-gamma))\n",
    "            #print(label)\n",
    "        \n",
    "        \n",
    "        images = images.astype(np.float32)\n",
    "        images /= 255\n",
    "        images = images.transpose(2, 0, 1)\n",
    "        \n",
    "        return torch.tensor(images), torch.tensor(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5000386523859661"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.beta(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_show = PANDADataset(df_train, image_size, n_tiles, 0, transform=None)\n",
    "images, label = dataset_show[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 3, 1536])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations by Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "transforms_train = albumentations.Compose([\n",
    "    albumentations.ShiftScaleRotate(scale_limit=0.25, rotate_limit=180,p=0.5),\n",
    "    A.OneOf([\n",
    "        A.HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit= 0.2, \n",
    "                             val_shift_limit=0.2, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, \n",
    "                                   contrast_limit=0.2, p=0.5),\n",
    "    ],p=0.9),\n",
    "    A.Cutout(num_holes=36, max_h_size=128, max_w_size=128, fill_value=0, p=0.5),\n",
    "    albumentations.Transpose(p=0.5),\n",
    "    albumentations.VerticalFlip(p=0.5),\n",
    "    albumentations.HorizontalFlip(p=0.5),   \n",
    "])\n",
    "transforms_val = albumentations.Compose([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.2.0) /io/opencv/modules/core/src/copy.cpp:998: error: (-215:Assertion failed) _src.dims() <= 2 in function 'flip'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_37026/1833843652.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_show\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_show\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0maxarr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_37026/3597507179.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m# aug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# Mixup part\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pca/lib/python3.7/site-packages/albumentations/core/composition.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **data)\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_apply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdual_start_end\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mdual_start_end\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pca/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                     )\n\u001b[1;32m     86\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_with_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pca/lib/python3.7/site-packages/albumentations/core/transforms_interface.py\u001b[0m in \u001b[0;36mapply_with_params\u001b[0;34m(self, params, force_apply, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mtarget_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_target_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mtarget_dependencies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_dependence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtarget_dependencies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pca/lib/python3.7/site-packages/albumentations/augmentations/transforms.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, img, **params)\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0;31m# Opencv is faster than numpy only in case of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;31m# non-gray scale 8bits images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pca/lib/python3.7/site-packages/albumentations/augmentations/functional.py\u001b[0m in \u001b[0;36mhflip_cv2\u001b[0;34m(img)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mhflip_cv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.2.0) /io/opencv/modules/core/src/copy.cpp:998: error: (-215:Assertion failed) _src.dims() <= 2 in function 'flip'\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_show = PANDADataset(df_train, image_size, n_tiles, 0, transform=transforms_train)\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20,10\n",
    "for i in range(3):\n",
    "    f, axarr = plt.subplots(1,5)\n",
    "    for p in range(5):\n",
    "        idx = np.random.randint(0, len(dataset_show))\n",
    "        img, label = dataset_show[idx]\n",
    "        axarr[p].imshow(1. - img.transpose(0, 1).transpose(1,2).squeeze())\n",
    "        axarr[p].set_title(str(sum(label)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(loader, optimizer):\n",
    "\n",
    "    model.eval()\n",
    "    train_loss = []\n",
    "    bar = tqdm(loader)\n",
    "    for (data, target) in bar:\n",
    "        \n",
    "        data, target = data.to(device), target.to(device)\n",
    "        loss_func = criterion\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_np = loss.detach().cpu().numpy()\n",
    "        train_loss.append(loss_np)\n",
    "        smooth_loss = sum(train_loss[-100:]) / min(len(train_loss), 100)\n",
    "        bar.set_description('loss: %.5f, smth: %.5f' % (loss_np, smooth_loss))\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def val_epoch(loader, get_output=False):\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    LOGITS = []\n",
    "    PREDS = []\n",
    "    TARGETS = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (data, target) in tqdm(loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            logits = model(data)\n",
    "\n",
    "            loss = criterion(logits, target)\n",
    "\n",
    "            pred = logits.sigmoid().sum(1).detach().round()\n",
    "            LOGITS.append(logits)\n",
    "            PREDS.append(pred)\n",
    "            TARGETS.append(target.sum(1))\n",
    "\n",
    "            val_loss.append(loss.detach().cpu().numpy())\n",
    "        val_loss = np.mean(val_loss)\n",
    "\n",
    "    LOGITS = torch.cat(LOGITS).cpu().numpy()\n",
    "    PREDS = torch.cat(PREDS).cpu().numpy()\n",
    "    TARGETS = torch.cat(TARGETS).cpu().numpy()\n",
    "    acc = (PREDS == TARGETS).mean() * 100.\n",
    "    \n",
    "    qwk = cohen_kappa_score(PREDS, TARGETS, weights='quadratic')\n",
    "    qwk_k = cohen_kappa_score(PREDS[df_valid['data_provider'] == 'karolinska'], df_valid[df_valid['data_provider'] == 'karolinska'].isup_grade.values, weights='quadratic')\n",
    "    qwk_r = cohen_kappa_score(PREDS[df_valid['data_provider'] == 'radboud'], df_valid[df_valid['data_provider'] == 'radboud'].isup_grade.values, weights='quadratic')\n",
    "    print('qwk', qwk, 'qwk_k', qwk_k, 'qwk_r', qwk_r)\n",
    "    \n",
    "    if EXP:\n",
    "        scheduler.step(val_loss)\n",
    "    \n",
    "    if get_output:\n",
    "        return LOGITS\n",
    "    else:\n",
    "        return val_loss, acc, qwk, qwk_k, qwk_r, PREDS, TARGETS\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataloader & Model & Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-fold models are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in range(0,5):\n",
    "    train_idx = np.where((df_train['fold'] != fold))[0]\n",
    "    valid_idx = np.where((df_train['fold'] == fold))[0]\n",
    "\n",
    "    df_this  = df_train.loc[train_idx]\n",
    "    df_valid = df_train.loc[valid_idx]\n",
    "\n",
    "    df_this = df_this.reset_index()\n",
    "    df_valid = df_valid.reset_index()\n",
    "\n",
    "    dataset_train = PANDADataset(df_this , image_size, n_tiles, transform=transforms_train)\n",
    "    dataset_valid = PANDADataset(df_valid, image_size, n_tiles, transform=None)\n",
    "    \n",
    "    # Setup dataloader\n",
    "    train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, sampler=RandomSampler(dataset_train), num_workers=num_workers)\n",
    "    valid_loader = torch.utils.data.DataLoader(dataset_valid, batch_size=batch_size, sampler=SequentialSampler(dataset_valid), num_workers=num_workers)\n",
    "\n",
    "    # Initialize model\n",
    "    model = enetv2(out_dim=out_dim)\n",
    "    model = model.to(device)\n",
    "\n",
    "    print(len(dataset_train), len(dataset_valid))\n",
    "\n",
    "    # We use Cosine annealing LR scheduling\n",
    "    if COSINE:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=init_lr/warmup_factor)\n",
    "        scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, n_epochs-warmup_epo)\n",
    "        scheduler = GradualWarmupScheduler(optimizer, multiplier=warmup_factor, total_epoch=warmup_epo, after_scheduler=scheduler_cosine)\n",
    "    else:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=init_lr)\n",
    "        from torch.optim import lr_scheduler\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=4, verbose=True, min_lr=1e-3*1e-5, factor=0.5)\n",
    "\n",
    "    qwk_max = 0.\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"log\", exist_ok=True)\n",
    "    best_file = f'./models/{kernel_type}_best_fold{fold}.pth'\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        torch.cuda.empty_cache() \n",
    "        print(time.ctime(), 'Epoch:', epoch)\n",
    "        if COSINE:\n",
    "            scheduler.step(epoch-1)\n",
    "\n",
    "        train_loss = train_epoch(train_loader, optimizer)\n",
    "        val_loss, acc, qwk, qwk_k, qwk_r, TARGETS, PREDS = val_epoch(valid_loader)\n",
    "\n",
    "        content = time.ctime() + ' ' + f'Epoch {epoch}, lr: {optimizer.param_groups[0][\"lr\"]:.7f}, train loss: {np.mean(train_loss):.5f}, val loss: {np.mean(val_loss):.5f}, acc: {(acc):.5f},     qwk: {(qwk):.5f}, qwk_k: {(qwk_k):.5f}, qwk_r: {(qwk_r):.5f}'\n",
    "        print(content)\n",
    "        with open(f'log/log_{kernel_type}_fold{fold}.txt', 'a') as appender:\n",
    "            appender.write(content + '\\n')\n",
    "        \n",
    "        from sklearn.metrics import cohen_kappa_score,confusion_matrix\n",
    "        cmat = confusion_matrix(TARGETS, PREDS).astype(\"uint\")\n",
    "        cmat\n",
    "\n",
    "        with open(f'log/logcmat_{kernel_type}_fold{fold}.txt', 'a') as appender:\n",
    "            appender.write(\"epoch:\" + str(epoch) + '\\n')\n",
    "            np.savetxt(appender, cmat, fmt='%3d')\n",
    "            \n",
    "        # use 20th epoch results as final to prevent overfitting.\n",
    "        if epoch==20:\n",
    "            torch.save(model.state_dict(), os.path.join(f'models/{kernel_type}_final_epoch{epoch}_fold{fold}.pth'))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pca]",
   "language": "python",
   "name": "conda-env-pca-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "117c736dca8349beb197997b569c520b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "1aba7eeb9d7040459953ee8cc1c57ddb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "282293ec10b14213911a555ab28f5273": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "32ead8e2299b47f69d264072a7830317": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "41156f41dccc48bb919888b37a171178": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_282293ec10b14213911a555ab28f5273",
       "placeholder": "",
       "style": "IPY_MODEL_1aba7eeb9d7040459953ee8cc1c57ddb",
       "value": " 10/10 [00:10&lt;00:00,  1.04s/it]"
      }
     },
     "465d0704ece943f99b00159e192738ad": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d3fb9db3ef8141e49d5cc78ba676ad17",
       "placeholder": "",
       "style": "IPY_MODEL_117c736dca8349beb197997b569c520b",
       "value": " 40/40 [00:42&lt;00:00,  1.06s/it]"
      }
     },
     "65ec03d6ea05405ba89d074994812ab4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cb5c37ea332e4db7a3c6e7ac0e8c2f47",
       "max": 10,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_32ead8e2299b47f69d264072a7830317",
       "value": 10
      }
     },
     "7566820607054425a234e25c418c0e49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e83d561f435460e8ff19ea062636702": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b28f9a83767a4fb1a086f4a2d98b08ca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "b8403be6607b4fc8a7b1fa25579bc970": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "loss: 0.68076, smth: 0.66723: 100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_8e83d561f435460e8ff19ea062636702",
       "max": 40,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_b28f9a83767a4fb1a086f4a2d98b08ca",
       "value": 40
      }
     },
     "cb5c37ea332e4db7a3c6e7ac0e8c2f47": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cfb548ddc9ee4774958243653d06cf26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b8403be6607b4fc8a7b1fa25579bc970",
        "IPY_MODEL_465d0704ece943f99b00159e192738ad"
       ],
       "layout": "IPY_MODEL_e83736ac502445259bd0fc4162e484cb"
      }
     },
     "d3fb9db3ef8141e49d5cc78ba676ad17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e83736ac502445259bd0fc4162e484cb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f1e9d76e042c4f269eeea3275804f577": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_65ec03d6ea05405ba89d074994812ab4",
        "IPY_MODEL_41156f41dccc48bb919888b37a171178"
       ],
       "layout": "IPY_MODEL_7566820607054425a234e25c418c0e49"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
