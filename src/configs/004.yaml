# Experiment result
#
#
#

General:
    comment: random rotate tile
    seed: 1222
    fp16: True
    # Compile level for AMP, O1, O2, O1 is better
    amp_level: O1
    # ddp, ddp2, dp, none
    # FP16 not support dp
    multi_gpu_mode: ddp
    gpus: [0]
    epoch: &epoch 25
    # Gradient accumulation option
    grad_acc: 1
    debug: False  # This value overwritten in train.py
    # TODO: implementation
#    save_weight: False

Data:
    dataset:
        train_df: ../input/train-10kfold.csv
        train_img_dir: ../input/train_images
        # kfold is overwritten in train.py
        kfold: -1
        random_tile_train: True
        rantom_rotate_tile: True
        remove_around_white: False
    dataloader:
        # If use_sampler is True, batch size of each loader is batch_size / (number of gpus)
        batch_size: 32
        num_workers: 4
        # These option is used for ddp
        train:
            use_sampler: True
            change_rate_sampler: False
            max_pos_rate: 1.0
        valid:
            use_sampler: True
        test:
            use_sampler: False

Model:
    base: resnet18
    # model_arch: normal, large_input
    model_arch: normal
    in_channel: 3
    out_channel: 1
    pretrained: True
    output: softmax

Optimizer:
    # optimizer = Adam(self.net.parameters(), lr=1e-4, amsgrad=False)
    # scheduler = ReduceLROnPlateau(
    #     optimizer, "min", factor=0.5, patience=2, verbose=True, eps=1e-6
    # )
    # return [optimizer], [scheduler]
    optimizer:
        name: Adam
        params:
            lr: !!python/float 1e-4
            amsgrad: False
    lr_scheduler:
        name: ReduceLROnPlateau
        params:
            mode: min
            factor: 0.5
            patience: 2
            verbose: True
            eps: !!python/float 1e-6

Loss:
    base_loss:
        name: BCEDiceLoss
        params:
            alpha: 0.7
            softmax: True
    # wrapper_loss: DiceScoreL2LossWrapper, ClsLossWrapper
    # These loss need in clsunet or msunet
    wrapper_loss:
        - name: ClsLossWrapper
          params:
              alpha: 0.1
              use_focal: True
              use_class0_cls: True
        - name: DiceScoreL2LossWrapper
          params:
              alpha: 0.2

Augmentation:
    # tta: hflip, vflip
    tta: []
    train:
        - name: HorizontalFlip
          params:
              p: 0.5
        - name: VerticalFlip
          params:
              p: 0.5
        - name: RandomRotate90
          params:
              p: 0.5
        - name: ShiftScaleRotate
          params:
              p: 0.5
              rotate_limit: 10
              shift_limit: 0.05
              scale_limit: 0.05
        - name: OneOf
          member:
              - name: ElasticTransform
                params:
                    alpha: 120
                    sigma: 6
                    alpha_affine: 3.6
              - name: GridDistortion
              - name: OpticalDistortion
                params:
                    distort_limit: 0.1
                    shift_limit: 0.1
          params:
              p: 0.5
        - name: Normalize
          params:
              p: 1.0
              mean: [0.86821552, 0.73319595, 0.81884091]
              std: [0.37468367, 0.5095725, 0.4131941]
    valid:
        - name: Normalize
          params:
              p: 1.0
              mean: [0.86821552, 0.73319595, 0.81884091]
              std: [0.37468367, 0.5095725, 0.4131941]
